{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "It is a generative learing algorithm and a popular classification algorithm .The algorithm makes the assumption that x's are conditionally independent of the y. This assumption is called Naive Bayes assumption.\n",
    "In this notebook we will use Naive Bayes to build an email spam filter that will classify messages according to whether they are unsolicited commercial (spam) email, or non-spam email. Classifying emails is one example of a broader set of problems called text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np #Importing Important Modules\n",
    "import math\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a dictionary with 2500 keys(words) and setting their value to 1. The reason of putting the value of 1\n",
    "instead of zero is because of the laplace smoothing of the numerator.If 0 was used then there would be a case that a word never occured in training but is present in the test case so at that point the probability will go to zero. This problem is fixed by laplace.After reading the data was pickled. Pickling helps faster reading of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic1={}          #dic1 contains words appeared in non spam emails.\n",
    "dic2={}          #dic2 contains words appeared in spam emails.\n",
    "for i in range(1,2501):\n",
    "    dic1.update({i:1})\n",
    "    dic2.update({i:1})\n",
    "k=[dic1,dic2]\n",
    "with open(\"dic.pickle\",\"wb\") as f:\n",
    "    pickle.dump(k,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"dic.pickle\",\"rb\") as f:\n",
    "    k=pickle.load(f)                                        #k[0] contains words appeared in non spam emails.\n",
    "                                                            #k[1] contains words appeared in spam emails.\n",
    "v=2500\n",
    "df=pd.read_csv(\"train-features.txt\", sep=' ',\n",
    "                  names = [\"DocID\", \"DicID\", \"Occ\"])\n",
    "s=df[\"DocID\"]\n",
    "\n",
    "#reading the file and giving them respective headers\n",
    "#DocId- Document number,DicID-Dictionary token number (1-2500),Occ-No. of times occured in the respective document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c variable is the DocId counter. The first value of DocId is 1.r is counting the total number of words occurances(occ) in a doc file. The loop is run to go through all the files. When the doc id changes the else part executes so then c value is incremented,r's value is appended in the list a. Now r is reset to 0 and the new r value of the next doc is added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Training the classifier\n",
    "c=1\n",
    "r=0                       #Counting the length of each words in the document\n",
    "a=[]                       #a is a list of all the lengths of document like a[0] is the no. of words in first document\n",
    "for i in range(len(s)):\n",
    "    if (s[i])==c:\n",
    "        r+=df[\"Occ\"][i]\n",
    "    else:\n",
    "        a.append(r)\n",
    "        c+=1                                     \n",
    "        r=r-r\n",
    "        r+=df[\"Occ\"][i] \n",
    "a.append(r)\n",
    "b=a[0:350]             #Dividing the lenghts into two lists. As 0-350 documents are not spam(0) and 350-700 are spam(1) \n",
    "a=a[350:700]\n",
    "nsp=sum(b)+v   #v is length of the dictionary ie 2500, it is added due to laplace smoothing\n",
    "sp=sum(a)+v\n",
    "sums=[nsp,sp]\n",
    "with open(\"dicsum.pickle\",\"wb\") as f:\n",
    "   pickle.dump(sums,f)                    #Pickling is used for faster loading of Data.\n",
    "\n",
    "sums=[]\n",
    "with open(\"dicsum.pickle\",\"rb\") as f:\n",
    "   sums=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k[0] is the not-spam dictionary and k[1] is the spam dictionary in which the occurances of each word is added according to the 2500 keys(words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(s)):              #Updating the non spam and spam dictionary by adding the occurance of the word.\n",
    "    if int(s[i])<=350:\n",
    "        k[0][(df[\"DicID\"][i])]+=df[\"Occ\"][i]\n",
    "    else:\n",
    "        k[1][(df[\"DicID\"][i])]+=df[\"Occ\"][i]\n",
    "            \n",
    "with open(\"classydicl.pickle\",\"wb\") as f:\n",
    "   pickle.dump(k,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"classydicl.pickle\",\"rb\") as f:\n",
    "    q=pickle.load(f)                    #Our numerator and denominator are both ready.Now we Divide.\n",
    "\n",
    "for keys in (q[0]):\n",
    "    q[0][keys]=np.divide(q[0][keys],sums[0])\n",
    "    q[1][keys]=np.divide(q[1][keys],sums[1])\n",
    "    \n",
    "\n",
    "with open(\"newclassydic.pickle\",\"wb\") as f:\n",
    "   pickle.dump(q,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"newclassydic.pickle\",\"rb\") as f:\n",
    "    k=pickle.load(f)\n",
    "#newclassydic is our trained classifier\n",
    "#k loads the new classifier k[0] contains non spam and k[1] contains spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier has been trained on the training dataset.There are 50% spam and 50% non-spam labels in our training dataset.Log is used to prevent underflow errors.It's time to check accuracy on the test data set. The variable e calculates probability of not-spam and f calculates the probability of spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Naive Bayes Algorithm is 98.46153846153847\n"
     ]
    }
   ],
   "source": [
    "##Testing The Naive Bayes Classifier\n",
    "\n",
    "df=pd.read_csv(\"test-features.txt\", sep=' ',\n",
    "                  names = [\"DocID\", \"DicID\", \"Occ\"])  #reading the file and giving them respective headers\n",
    "s=df[\"DocID\"]\n",
    "t=df[\"DicID\"]\n",
    "u=df[\"Occ\"]\n",
    "x=np.log(0.50)                  #0.50 is the probability of spam and non spam dataset in our training data.\n",
    "y=np.log(0.50)                  #x is the prob of non spam and y is the prob of of spam\n",
    "                                                   #Applying the naive bayes algorithm.We are adding the log instead of multipying due to underflow.\n",
    "z=1\n",
    "arr=[]\n",
    "for i in range(len(s)):\n",
    "    if (s[i]==z):\n",
    "        e=(k[0][t[i]])*(u[i])\n",
    "        f=(k[1][t[i]])*(u[i])\n",
    "        x+=np.log(e)\n",
    "        y+=np.log(f)\n",
    "    else:\n",
    "        z+=1\n",
    "        if x>y:\n",
    "            arr.append(0)\n",
    "        else:\n",
    "            arr.append(1)\n",
    "        x=np.log(0.50)\n",
    "        y=np.log(0.50)\n",
    "        e=(k[0][t[i]])*(u[i])\n",
    "        f=(k[1][t[i]])*(u[i])\n",
    "        x+=np.log(e)\n",
    "        y+=np.log(f)\n",
    "if x>y:\n",
    "    arr.append(0)\n",
    "else:\n",
    "    arr.append(1)\n",
    "df=pd.read_csv(\"test-labels.txt\",names = [\"LabelId\"])  #reading the file and giving them respective header.\n",
    "accuracy=0\n",
    "l=df[\"LabelId\"]\n",
    "for i in range(len(arr)):      #Comparing test label and prediction(arr)\n",
    "    if (l[i]==arr[i]):\n",
    "        accuracy+=1\n",
    "accuracy=accuracy/len(arr)\n",
    "print (\"Accuracy of the Naive Bayes Algorithm is\",accuracy*100.0)\n",
    "submission = pd.DataFrame(arr)\n",
    "submission.to_csv('prediction.txt',index = False)#Creates prediction into a new file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
