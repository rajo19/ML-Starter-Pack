{
  "cells": [
    {
      "metadata": {
        "_uuid": "07d904f2e982955aba8e4fa4ddd6e595df462b2e"
      },
      "cell_type": "markdown",
      "source": "**Pytorch Neural Network Implemetation using torch.nn module**"
    },
    {
      "metadata": {
        "_uuid": "397e10433256c36e6226954a36ed60b3802b1b22"
      },
      "cell_type": "markdown",
      "source": "In this notebook I am going to build a neural netwprk model using pytorch torch.nn module.It is quite simple to develop this. One just has to know the basics of the nn model and not much computation is required for pytorch implementation as torch.nn module has everything that a nn model needs satrting from forward to backeard propagation."
    },
    {
      "metadata": {
        "_cell_guid": "9f6722f3-2a1b-4bb9-8ed5-77eb0ca55a2b",
        "_uuid": "6f9a127a6654bb4d278b5198d170dc94393277d6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": "column_2C_weka.csv\ncolumn_3C_weka.csv\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2632b3025a2c02e22dddaf044c490b9b71bfa4fe"
      },
      "cell_type": "markdown",
      "source": "We will be using Biomechanical Orthopedic Patients data set to classifiy using our neural network model.This data set has 310 training examples with 6 features and one target attribute which is 'class'."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e304a742228b6505728446f7c12c9aafe38b77ef"
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv('../input/column_2C_weka.csv')\ntrain_df.head()",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 129,
          "data": {
            "text/plain": "   pelvic_incidence  pelvic_tilt numeric  lumbar_lordosis_angle  sacral_slope  \\\n0         63.027818            22.552586              39.609117     40.475232   \n1         39.056951            10.060991              25.015378     28.995960   \n2         68.832021            22.218482              50.092194     46.613539   \n3         69.297008            24.652878              44.311238     44.644130   \n4         49.712859             9.652075              28.317406     40.060784   \n\n   pelvic_radius  degree_spondylolisthesis     class  \n0      98.672917                 -0.254400  Abnormal  \n1     114.405425                  4.564259  Abnormal  \n2     105.985135                 -3.530317  Abnormal  \n3     101.868495                 11.211523  Abnormal  \n4     108.168725                  7.918501  Abnormal  ",
            "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pelvic_incidence</th>\n      <th>pelvic_tilt numeric</th>\n      <th>lumbar_lordosis_angle</th>\n      <th>sacral_slope</th>\n      <th>pelvic_radius</th>\n      <th>degree_spondylolisthesis</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63.027818</td>\n      <td>22.552586</td>\n      <td>39.609117</td>\n      <td>40.475232</td>\n      <td>98.672917</td>\n      <td>-0.254400</td>\n      <td>Abnormal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39.056951</td>\n      <td>10.060991</td>\n      <td>25.015378</td>\n      <td>28.995960</td>\n      <td>114.405425</td>\n      <td>4.564259</td>\n      <td>Abnormal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>68.832021</td>\n      <td>22.218482</td>\n      <td>50.092194</td>\n      <td>46.613539</td>\n      <td>105.985135</td>\n      <td>-3.530317</td>\n      <td>Abnormal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>69.297008</td>\n      <td>24.652878</td>\n      <td>44.311238</td>\n      <td>44.644130</td>\n      <td>101.868495</td>\n      <td>11.211523</td>\n      <td>Abnormal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>49.712859</td>\n      <td>9.652075</td>\n      <td>28.317406</td>\n      <td>40.060784</td>\n      <td>108.168725</td>\n      <td>7.918501</td>\n      <td>Abnormal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a068a8ab314f5e013f13286595b838987288c25c"
      },
      "cell_type": "markdown",
      "source": "Here we import torch and Variable class from torch.autograd which simplifies our job of finding the gardients of loss function during backpropagation. To use torch.nn module we need train_x ant target to be Variable objects not simply torch.Tensors. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "d429a08c9878c42b0f9ff0e0e128241b7613d970"
      },
      "cell_type": "code",
      "source": "import torch\nfrom torch.autograd import Variable\ndtype = torch.FloatTensor",
      "execution_count": 130,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5d0c91b602638f98ee30ee2faf7c4f1442ca4c9f"
      },
      "cell_type": "markdown",
      "source": "This part shows some data preprocessing steps that include mapping the strings to int and forming the training data train_x as Variavle form numpy ndarray."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "dd612296314f327b228ab7debd5ea30e7464eef6"
      },
      "cell_type": "code",
      "source": "dict1 = {'Normal':0 , 'Abnormal':1}\ntrain_df['class'] = train_df['class'].map(dict1)\ntarget = train_df['class'].values\ndel train_df['class']\ntrain_x = train_df.as_matrix()\ntrain_x = Variable(torch.from_numpy(train_x).type(dtype))\ntarget = Variable(torch.from_numpy(target).type(dtype))",
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1daea3230815a9e678f9d8211e7623542580b050"
      },
      "cell_type": "markdown",
      "source": "This is the initialisation part. Here we define the size of our nn model ,which is one hidden layer of size 10 in this example. Then we define the nn model by craeting a object nn_model using torch.nn.Sequential where sequentially we add layers to the model. In our model first we have an input layer of size 6 ,then a linear function applied on the input layer having parameter dimension (n_input,n_hidden) . Then a ReLU applied to that hidden layer and a linear function again produces the fianl layer and a sigmoid is finally applies on that fianl layer to produce the y_pred or the output probabilities."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "33a19513d7ac2b4bb9865e667931035ee187d44c"
      },
      "cell_type": "code",
      "source": "m = train_df.shape[1]\nn_input = m\nn_hidden = 10\nn_output = 1\nnn_model = torch.nn.Sequential(\n    torch.nn.Linear(n_input,n_hidden),\n    torch.nn.ReLU(),\n    torch.nn.Linear(n_hidden,n_output),\n    torch.nn.Sigmoid())\nloss_fn = torch.nn.BCELoss() ## creating an object of Binary Cross entropy loss \nalpha = .002 ## learning rate\nn_iter = 1000 ## no of iterations",
      "execution_count": 131,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d5c41365dcbdbb1d0235eae9ebf7326027f0133e"
      },
      "cell_type": "markdown",
      "source": "Here is our actaul algorithm implementation. We first compute the y_pred using our existing params.Then find loss.Then using loss.backward get the gradients . Then finally upadate the parameters in nn_model which is contained in nn_model.parameters(). "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21d1655db053085d2ace95031cba7aafee63dd47"
      },
      "cell_type": "code",
      "source": "cost = []\niter1 = []\ny_pred \nfor t in range(n_iter):\n    ## forward propagation to compute the predicted output y_pred form input data train_x\n    y_pred = nn_model(train_x)\n    \n    ## computing the cost\n    loss = loss_fn(y_pred,target)\n    print('iter'+str(t)+' loss = '+str(loss.data[0]))\n    cost.append(loss.data[0])\n    iter1.append(t)\n    \n    ## inialize the grads to zero before computing them\n    nn_model.zero_grad()\n    \n    ## computes the gradients and stores them to param.grad\n    loss.backward()\n    \n    ## updating the params\n    for param in nn_model.parameters():\n        param.data =  param.data - alpha * param.grad.data",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([310])) that is different to the input size (torch.Size([310, 1])) is deprecated. Please ensure they have the same size.\n  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "iter0 loss = 2.4455535411834717\niter1 loss = 4.180837631225586\niter2 loss = 1.5746468305587769\niter3 loss = 1.538549780845642\niter4 loss = 3.4395575523376465\niter5 loss = 1.0011605024337769\niter6 loss = 1.1787807941436768\niter7 loss = 1.8896273374557495\niter8 loss = 0.5670288801193237\niter9 loss = 0.6432705521583557\niter10 loss = 0.7850519418716431\niter11 loss = 0.9378297924995422\niter12 loss = 0.9240021705627441\niter13 loss = 1.002001166343689\niter14 loss = 0.8752433657646179\niter15 loss = 0.8622692227363586\niter16 loss = 0.8420873880386353\niter17 loss = 0.7924823760986328\niter18 loss = 0.8018447160720825\niter19 loss = 0.7422419190406799\niter20 loss = 0.7664347290992737\niter21 loss = 0.7093923687934875\niter22 loss = 0.739213228225708\niter23 loss = 0.6879369616508484\niter24 loss = 0.7199928164482117\niter25 loss = 0.6743884086608887\niter26 loss = 0.707064688205719\niter27 loss = 0.6657314896583557\niter28 loss = 0.6982100605964661\niter29 loss = 0.6598170399665833\niter30 loss = 0.6916256546974182\niter31 loss = 0.6552169919013977\niter32 loss = 0.6860623955726624\niter33 loss = 0.6510590314865112\niter34 loss = 0.6807706356048584\niter35 loss = 0.6471707224845886\niter36 loss = 0.6752862930297852\niter37 loss = 0.6421074271202087\niter38 loss = 0.6689728498458862\niter39 loss = 0.6367350220680237\niter40 loss = 0.6625197529792786\niter41 loss = 0.6314038038253784\niter42 loss = 0.6564569473266602\niter43 loss = 0.62690269947052\niter44 loss = 0.6511149406433105\niter45 loss = 0.6227700710296631\niter46 loss = 0.6460562944412231\niter47 loss = 0.618716835975647\niter48 loss = 0.6409919857978821\niter49 loss = 0.6144479513168335\niter50 loss = 0.6357747912406921\niter51 loss = 0.6100729703903198\niter52 loss = 0.6305128931999207\niter53 loss = 0.6056785583496094\niter54 loss = 0.6252841353416443\niter55 loss = 0.6013256907463074\niter56 loss = 0.6201308369636536\niter57 loss = 0.5971899032592773\niter58 loss = 0.6150158047676086\niter59 loss = 0.5926982164382935\niter60 loss = 0.6097057461738586\niter61 loss = 0.5881256461143494\niter62 loss = 0.6045445799827576\niter63 loss = 0.5839208364486694\niter64 loss = 0.5997450947761536\niter65 loss = 0.579967200756073\niter66 loss = 0.5950924158096313\niter67 loss = 0.5759243369102478\niter68 loss = 0.5904396176338196\niter69 loss = 0.571922779083252\niter70 loss = 0.5858737826347351\niter71 loss = 0.5682419538497925\niter72 loss = 0.5813223719596863\niter73 loss = 0.5638269782066345\niter74 loss = 0.5763328075408936\niter75 loss = 0.5593736171722412\niter76 loss = 0.5716387033462524\niter77 loss = 0.5556051135063171\niter78 loss = 0.5675081014633179\niter79 loss = 0.552402138710022\niter80 loss = 0.5636818408966064\niter81 loss = 0.5491090416908264\niter82 loss = 0.5595561265945435\niter83 loss = 0.5452082753181458\niter84 loss = 0.5550327897071838\niter85 loss = 0.5407131910324097\niter86 loss = 0.5500836372375488\niter87 loss = 0.5366395115852356\niter88 loss = 0.5454268455505371\niter89 loss = 0.5325270891189575\niter90 loss = 0.5407603979110718\niter91 loss = 0.5282394289970398\niter92 loss = 0.5360894799232483\niter93 loss = 0.5244199633598328\niter94 loss = 0.5319602489471436\niter95 loss = 0.5209052562713623\niter96 loss = 0.5284023284912109\niter97 loss = 0.5181933641433716\niter98 loss = 0.5254902243614197\niter99 loss = 0.5158935785293579\niter100 loss = 0.5231708884239197\niter101 loss = 0.5142139792442322\niter102 loss = 0.5211763381958008\niter103 loss = 0.5125210285186768\niter104 loss = 0.5191174149513245\niter105 loss = 0.510661780834198\niter106 loss = 0.5168603658676147\niter107 loss = 0.508547306060791\niter108 loss = 0.5146215558052063\niter109 loss = 0.5063457489013672\niter110 loss = 0.5120401978492737\niter111 loss = 0.5039665699005127\niter112 loss = 0.5093234181404114\niter113 loss = 0.5014144778251648\niter114 loss = 0.5064764618873596\niter115 loss = 0.4985806941986084\niter116 loss = 0.5033909678459167\niter117 loss = 0.4957767128944397\niter118 loss = 0.500266432762146\niter119 loss = 0.4928680956363678\niter120 loss = 0.49714329838752747\niter121 loss = 0.49006378650665283\niter122 loss = 0.4941709041595459\niter123 loss = 0.4874015152454376\niter124 loss = 0.4912784695625305\niter125 loss = 0.48484891653060913\niter126 loss = 0.4885186553001404\niter127 loss = 0.4822474718093872\niter128 loss = 0.4859381914138794\niter129 loss = 0.48000359535217285\niter130 loss = 0.48361268639564514\niter131 loss = 0.47794532775878906\niter132 loss = 0.48145201802253723\niter133 loss = 0.4759950339794159\niter134 loss = 0.47937774658203125\niter135 loss = 0.4740931987762451\niter136 loss = 0.47734570503234863\niter137 loss = 0.4722384512424469\niter138 loss = 0.47555550932884216\niter139 loss = 0.470737487077713\niter140 loss = 0.4737483859062195\niter141 loss = 0.46882033348083496\niter142 loss = 0.47157996892929077\niter143 loss = 0.46681833267211914\niter144 loss = 0.46922942996025085\niter145 loss = 0.46439510583877563\niter146 loss = 0.4665640592575073\niter147 loss = 0.46179983019828796\niter148 loss = 0.46368712186813354\niter149 loss = 0.4590194821357727\niter150 loss = 0.460942804813385\niter151 loss = 0.4565136730670929\niter152 loss = 0.4586295783519745\niter153 loss = 0.45458516478538513\niter154 loss = 0.4567563831806183\niter155 loss = 0.4529663622379303\niter156 loss = 0.45515722036361694\niter157 loss = 0.4515551030635834\niter158 loss = 0.45392125844955444\niter159 loss = 0.4505404531955719\niter160 loss = 0.4529168903827667\niter161 loss = 0.4496549963951111\niter162 loss = 0.4521644413471222\niter163 loss = 0.4490295946598053\niter164 loss = 0.4515039622783661\niter165 loss = 0.44859570264816284\niter166 loss = 0.45086926221847534\niter167 loss = 0.44781726598739624\niter168 loss = 0.45003652572631836\niter169 loss = 0.4469410181045532\niter170 loss = 0.4491735100746155\niter171 loss = 0.44608569145202637\niter172 loss = 0.44821739196777344\niter173 loss = 0.44510507583618164\niter174 loss = 0.44711706042289734\niter175 loss = 0.4439908564090729\niter176 loss = 0.44587722420692444\niter177 loss = 0.4427248537540436\niter178 loss = 0.44449034333229065\niter179 loss = 0.4413486123085022\niter180 loss = 0.44300326704978943\niter181 loss = 0.4398724436759949\niter182 loss = 0.4414291977882385\niter183 loss = 0.43833038210868835\niter184 loss = 0.4397999942302704\niter185 loss = 0.43675270676612854\niter186 loss = 0.4383007287979126\niter187 loss = 0.435355544090271\niter188 loss = 0.4367345869541168\niter189 loss = 0.4338490068912506\niter190 loss = 0.43531081080436707\niter191 loss = 0.4325176179409027\niter192 loss = 0.43380436301231384\niter193 loss = 0.43107128143310547\niter194 loss = 0.4324367940425873\niter195 loss = 0.4297869801521301\niter196 loss = 0.43112999200820923\niter197 loss = 0.4285426437854767\niter198 loss = 0.42985445261001587\niter199 loss = 0.4273163378238678\niter200 loss = 0.42859044671058655\niter201 loss = 0.4261142313480377\niter202 loss = 0.42734596133232117\niter203 loss = 0.42490649223327637\niter204 loss = 0.4260910153388977\niter205 loss = 0.423910528421402\niter206 loss = 0.4249279499053955\niter207 loss = 0.4226345419883728\niter208 loss = 0.4235597252845764\niter209 loss = 0.4212936460971832\niter210 loss = 0.42227813601493835\niter211 loss = 0.42009449005126953\niter212 loss = 0.4209274351596832\niter213 loss = 0.41878244280815125\niter214 loss = 0.4195629358291626\niter215 loss = 0.41746965050697327\niter216 loss = 0.41809412837028503\niter217 loss = 0.41604071855545044\niter218 loss = 0.4166141748428345\niter219 loss = 0.4146236777305603\niter220 loss = 0.41516128182411194\niter221 loss = 0.4132477343082428\niter222 loss = 0.4139210879802704\niter223 loss = 0.41211193799972534\niter224 loss = 0.4127998352050781\niter225 loss = 0.4110720753669739\niter226 loss = 0.411759614944458\niter227 loss = 0.410107284784317\niter228 loss = 0.4107898473739624\niter229 loss = 0.409186989068985\niter230 loss = 0.40985843539237976\niter231 loss = 0.40829962491989136\niter232 loss = 0.4089542031288147\niter233 loss = 0.407426118850708\niter234 loss = 0.40805843472480774\niter235 loss = 0.40651026368141174\niter236 loss = 0.40713435411453247\niter237 loss = 0.40562060475349426\niter238 loss = 0.40623199939727783\niter239 loss = 0.40474650263786316\niter240 loss = 0.4053438603878021\niter241 loss = 0.4037669599056244\niter242 loss = 0.4043816924095154\niter243 loss = 0.40287429094314575\niter244 loss = 0.40349632501602173\niter245 loss = 0.4020252227783203\niter246 loss = 0.40264737606048584\niter247 loss = 0.40121835470199585\niter248 loss = 0.40183448791503906\niter249 loss = 0.400420606136322\niter250 loss = 0.4010251760482788\niter251 loss = 0.3996464014053345\niter252 loss = 0.4002378284931183\niter253 loss = 0.398865282535553\niter254 loss = 0.3993739187717438\niter255 loss = 0.3980088531970978\niter256 loss = 0.3984917402267456\niter257 loss = 0.3971279263496399\niter258 loss = 0.3975899815559387\niter259 loss = 0.39623478055000305\niter260 loss = 0.3966788947582245\niter261 loss = 0.3953387141227722\niter262 loss = 0.39576977491378784\niter263 loss = 0.3944477140903473\niter264 loss = 0.39486876130104065\niter265 loss = 0.39356812834739685\niter266 loss = 0.39398193359375\niter267 loss = 0.3927196264266968\niter268 loss = 0.39312848448753357\niter269 loss = 0.3918876051902771\niter270 loss = 0.39228853583335876\niter271 loss = 0.39106956124305725\niter272 loss = 0.3914668560028076\niter273 loss = 0.3902696669101715\niter274 loss = 0.39066269993782043\niter275 loss = 0.3894869387149811\niter276 loss = 0.3898683488368988\niter277 loss = 0.38885751366615295\niter278 loss = 0.389244019985199\niter279 loss = 0.3881725072860718\niter280 loss = 0.38854578137397766\niter281 loss = 0.3876263499259949\niter282 loss = 0.38786131143569946\niter283 loss = 0.38685160875320435\niter284 loss = 0.38705888390541077\niter285 loss = 0.38603484630584717\niter286 loss = 0.386313259601593\niter287 loss = 0.3853229880332947\niter288 loss = 0.38560593128204346\niter289 loss = 0.38462525606155396\niter290 loss = 0.38483554124832153\niter291 loss = 0.3838651478290558\niter292 loss = 0.3840954899787903\niter293 loss = 0.38308191299438477\niter294 loss = 0.38333699107170105\niter295 loss = 0.38233450055122375\niter296 loss = 0.3826151490211487\niter297 loss = 0.3816230893135071\niter298 loss = 0.38191866874694824\niter299 loss = 0.38104891777038574\niter300 loss = 0.3813447058200836\niter301 loss = 0.3805771768093109\niter302 loss = 0.38085269927978516\niter303 loss = 0.38002532720565796\niter304 loss = 0.3802824020385742\niter305 loss = 0.37956032156944275\niter306 loss = 0.37960284948349\niter307 loss = 0.37883132696151733\niter308 loss = 0.3788757920265198\niter309 loss = 0.37804698944091797\niter310 loss = 0.37810298800468445\niter311 loss = 0.37724024057388306\niter312 loss = 0.37731504440307617\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "iter313 loss = 0.37643569707870483\niter314 loss = 0.3765300512313843\niter315 loss = 0.37556931376457214\niter316 loss = 0.37558335065841675\niter317 loss = 0.3748873472213745\niter318 loss = 0.3750012516975403\niter319 loss = 0.3742693066596985\niter320 loss = 0.3742941915988922\niter321 loss = 0.37354224920272827\niter322 loss = 0.37365826964378357\niter323 loss = 0.3730714023113251\niter324 loss = 0.37316662073135376\niter325 loss = 0.3725162744522095\niter326 loss = 0.37269484996795654\niter327 loss = 0.3721812665462494\niter328 loss = 0.3723251521587372\niter329 loss = 0.3717781901359558\niter330 loss = 0.3719073534011841\niter331 loss = 0.3713773488998413\niter332 loss = 0.3714885413646698\niter333 loss = 0.3709773123264313\niter334 loss = 0.3710768520832062\niter335 loss = 0.3705846667289734\niter336 loss = 0.37067753076553345\niter337 loss = 0.3701534569263458\niter338 loss = 0.3701656758785248\niter339 loss = 0.3696282207965851\niter340 loss = 0.3695622384548187\niter341 loss = 0.3689848482608795\niter342 loss = 0.3689628839492798\niter343 loss = 0.36835429072380066\niter344 loss = 0.3683775067329407\niter345 loss = 0.3677440881729126\niter346 loss = 0.3678051233291626\niter347 loss = 0.36725860834121704\niter348 loss = 0.3673590421676636\niter349 loss = 0.36690792441368103\niter350 loss = 0.3669927716255188\niter351 loss = 0.3665415048599243\niter352 loss = 0.366699755191803\niter353 loss = 0.3662586510181427\niter354 loss = 0.366409033536911\niter355 loss = 0.36608073115348816\niter356 loss = 0.3661896884441376\niter357 loss = 0.3658115267753601\niter358 loss = 0.36589911580085754\niter359 loss = 0.3655356764793396\niter360 loss = 0.3656780421733856\niter361 loss = 0.36527276039123535\niter362 loss = 0.3653395175933838\niter363 loss = 0.36492806673049927\niter364 loss = 0.36500492691993713\niter365 loss = 0.36457985639572144\niter366 loss = 0.3645307123661041\niter367 loss = 0.36406201124191284\niter368 loss = 0.3640449345111847\niter369 loss = 0.36354804039001465\niter370 loss = 0.36362579464912415\niter371 loss = 0.3631373643875122\niter372 loss = 0.36346665024757385\niter373 loss = 0.36300569772720337\niter374 loss = 0.3633278012275696\niter375 loss = 0.36297905445098877\niter376 loss = 0.3634609282016754\niter377 loss = 0.36312413215637207\niter378 loss = 0.3636285066604614\niter379 loss = 0.3632940649986267\niter380 loss = 0.36381298303604126\niter381 loss = 0.36347243189811707\niter382 loss = 0.3639981746673584\niter383 loss = 0.36364519596099854\niter384 loss = 0.3641708493232727\niter385 loss = 0.36379700899124146\niter386 loss = 0.36431553959846497\niter387 loss = 0.36392468214035034\niter388 loss = 0.36443039774894714\niter389 loss = 0.36400964856147766\niter390 loss = 0.3644973337650299\niter391 loss = 0.36404481530189514\niter392 loss = 0.36451032757759094\niter393 loss = 0.36402949690818787\niter394 loss = 0.3646072745323181\niter395 loss = 0.364095002412796\niter396 loss = 0.36465975642204285\niter397 loss = 0.36420223116874695\niter398 loss = 0.36456814408302307\niter399 loss = 0.36428359150886536\niter400 loss = 0.36453694105148315\niter401 loss = 0.36421874165534973\niter402 loss = 0.3643760681152344\niter403 loss = 0.36393916606903076\niter404 loss = 0.36403560638427734\niter405 loss = 0.36350154876708984\niter406 loss = 0.36356067657470703\niter407 loss = 0.3629074692726135\niter408 loss = 0.36280739307403564\niter409 loss = 0.36215201020240784\niter410 loss = 0.36204126477241516\niter411 loss = 0.3613935708999634\niter412 loss = 0.3612774908542633\niter413 loss = 0.3606419265270233\niter414 loss = 0.36040350794792175\niter415 loss = 0.35976600646972656\niter416 loss = 0.3595420718193054\niter417 loss = 0.35891202092170715\niter418 loss = 0.35871177911758423\niter419 loss = 0.3580177128314972\niter420 loss = 0.3578418791294098\niter421 loss = 0.35717877745628357\niter422 loss = 0.3570246696472168\niter423 loss = 0.3563899099826813\niter424 loss = 0.35625821352005005\niter425 loss = 0.35564953088760376\niter426 loss = 0.35553982853889465\niter427 loss = 0.3549404442310333\niter428 loss = 0.3548505902290344\niter429 loss = 0.354275107383728\niter430 loss = 0.3542034924030304\niter431 loss = 0.3536487817764282\niter432 loss = 0.353594571352005\niter433 loss = 0.35305729508399963\niter434 loss = 0.3530194163322449\niter435 loss = 0.3525038957595825\niter436 loss = 0.35247740149497986\niter437 loss = 0.35208871960639954\niter438 loss = 0.3520790934562683\niter439 loss = 0.3516712188720703\niter440 loss = 0.3516780734062195\niter441 loss = 0.35125496983528137\niter442 loss = 0.3512784242630005\niter443 loss = 0.3509651720523834\niter444 loss = 0.3509915769100189\niter445 loss = 0.3507789969444275\niter446 loss = 0.350783109664917\niter447 loss = 0.3505491316318512\niter448 loss = 0.3505348265171051\niter449 loss = 0.3502851724624634\niter450 loss = 0.3502558171749115\niter451 loss = 0.3500101864337921\niter452 loss = 0.35009971261024475\niter453 loss = 0.3498630225658417\niter454 loss = 0.34994590282440186\niter455 loss = 0.34971725940704346\niter456 loss = 0.34979304671287537\niter457 loss = 0.3495708405971527\niter458 loss = 0.34963902831077576\niter459 loss = 0.3494231700897217\niter460 loss = 0.349483847618103\niter461 loss = 0.3492734730243683\niter462 loss = 0.34932687878608704\niter463 loss = 0.3491261303424835\niter464 loss = 0.3492550253868103\niter465 loss = 0.34905770421028137\niter466 loss = 0.34918585419654846\niter467 loss = 0.3489898145198822\niter468 loss = 0.349115788936615\niter469 loss = 0.3489200472831726\niter470 loss = 0.34904244542121887\niter471 loss = 0.34884610772132874\niter472 loss = 0.34895646572113037\niter473 loss = 0.3487587571144104\niter474 loss = 0.34886178374290466\niter475 loss = 0.34866154193878174\niter476 loss = 0.3487567901611328\niter477 loss = 0.3485598564147949\niter478 loss = 0.34864601492881775\niter479 loss = 0.34844616055488586\niter480 loss = 0.34852296113967896\niter481 loss = 0.3483199179172516\niter482 loss = 0.34838685393333435\niter483 loss = 0.3481806218624115\niter484 loss = 0.3482377827167511\niter485 loss = 0.34802889823913574\niter486 loss = 0.3480757772922516\niter487 loss = 0.3478666841983795\niter488 loss = 0.3479035794734955\niter489 loss = 0.3476923406124115\niter490 loss = 0.3477194607257843\niter491 loss = 0.3475068509578705\niter492 loss = 0.34752410650253296\niter493 loss = 0.34731075167655945\niter494 loss = 0.34731894731521606\niter495 loss = 0.3471052348613739\niter496 loss = 0.34710443019866943\niter497 loss = 0.34689104557037354\niter498 loss = 0.34688207507133484\niter499 loss = 0.34666958451271057\niter500 loss = 0.34665295481681824\niter501 loss = 0.3463718891143799\niter502 loss = 0.3463582396507263\niter503 loss = 0.34615033864974976\niter504 loss = 0.3461293578147888\niter505 loss = 0.34585297107696533\niter506 loss = 0.3458341360092163\niter507 loss = 0.345562607049942\niter508 loss = 0.34554487466812134\niter509 loss = 0.3452775180339813\niter510 loss = 0.3452610969543457\niter511 loss = 0.3450639247894287\niter512 loss = 0.34503868222236633\niter513 loss = 0.34477537870407104\niter514 loss = 0.3447514772415161\niter515 loss = 0.344492644071579\niter516 loss = 0.34446966648101807\niter517 loss = 0.34421488642692566\niter518 loss = 0.3441922664642334\niter519 loss = 0.3439413905143738\niter520 loss = 0.343919038772583\niter521 loss = 0.34373635053634644\niter522 loss = 0.3437059819698334\niter523 loss = 0.3434600234031677\niter524 loss = 0.3434283137321472\niter525 loss = 0.34319034218788147\niter526 loss = 0.3431594967842102\niter527 loss = 0.3429255783557892\niter528 loss = 0.34289583563804626\niter529 loss = 0.3426657021045685\niter530 loss = 0.3426363170146942\niter531 loss = 0.34241026639938354\niter532 loss = 0.34238094091415405\niter533 loss = 0.34215864539146423\niter534 loss = 0.34212949872016907\niter535 loss = 0.34191110730171204\niter536 loss = 0.3418823182582855\niter537 loss = 0.34166765213012695\niter538 loss = 0.3416389226913452\niter539 loss = 0.3414280414581299\niter540 loss = 0.34139907360076904\niter541 loss = 0.3411918580532074\niter542 loss = 0.3411630094051361\niter543 loss = 0.3410172760486603\niter544 loss = 0.3409818112850189\niter545 loss = 0.34078094363212585\niter546 loss = 0.34074705839157104\niter547 loss = 0.3405499756336212\niter548 loss = 0.3405177891254425\niter549 loss = 0.34032487869262695\niter550 loss = 0.34029388427734375\niter551 loss = 0.3401045799255371\niter552 loss = 0.3400748074054718\niter553 loss = 0.33988916873931885\niter554 loss = 0.33986058831214905\niter555 loss = 0.33967822790145874\niter556 loss = 0.3396509289741516\niter557 loss = 0.3394719064235687\niter558 loss = 0.3394458591938019\niter559 loss = 0.3392699658870697\niter560 loss = 0.3392450511455536\niter561 loss = 0.3390727639198303\niter562 loss = 0.338996946811676\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "iter563 loss = 0.33883151412010193\niter564 loss = 0.3388036787509918\niter565 loss = 0.3386421501636505\niter566 loss = 0.3385675549507141\niter567 loss = 0.3384120464324951\niter568 loss = 0.33838436007499695\niter569 loss = 0.3382328748703003\niter570 loss = 0.3381606936454773\niter571 loss = 0.3380149006843567\niter572 loss = 0.33794301748275757\niter573 loss = 0.3378039300441742\niter574 loss = 0.33777546882629395\niter575 loss = 0.337639719247818\niter576 loss = 0.33757126331329346\niter577 loss = 0.33744069933891296\niter578 loss = 0.337373286485672\niter579 loss = 0.3372478187084198\niter580 loss = 0.3372214436531067\niter581 loss = 0.33709999918937683\niter582 loss = 0.3370177745819092\niter583 loss = 0.3369012475013733\niter584 loss = 0.3368373215198517\niter585 loss = 0.33672553300857544\niter586 loss = 0.33669954538345337\niter587 loss = 0.3365916311740875\niter588 loss = 0.33651548624038696\niter589 loss = 0.3364117741584778\niter590 loss = 0.33633697032928467\niter591 loss = 0.3362381160259247\niter592 loss = 0.33619818091392517\niter593 loss = 0.3361024856567383\niter594 loss = 0.33603164553642273\niter595 loss = 0.3359401822090149\niter596 loss = 0.33587101101875305\niter597 loss = 0.3357841372489929\niter598 loss = 0.3357463777065277\niter599 loss = 0.33566176891326904\niter600 loss = 0.3355966806411743\niter601 loss = 0.3355155885219574\niter602 loss = 0.33545199036598206\niter603 loss = 0.33537593483924866\niter604 loss = 0.3353414237499237\niter605 loss = 0.3352673351764679\niter606 loss = 0.3352072238922119\niter607 loss = 0.33513617515563965\niter608 loss = 0.33507785201072693\niter609 loss = 0.33500993251800537\niter610 loss = 0.334953248500824\niter611 loss = 0.3348879814147949\niter612 loss = 0.3348329961299896\niter613 loss = 0.3347702622413635\niter614 loss = 0.3347398042678833\niter615 loss = 0.3346787095069885\niter616 loss = 0.3346267342567444\niter617 loss = 0.33456772565841675\niter618 loss = 0.33451732993125916\niter619 loss = 0.33446019887924194\niter620 loss = 0.3344113528728485\niter621 loss = 0.3343559503555298\niter622 loss = 0.3343084156513214\niter623 loss = 0.3342549800872803\niter624 loss = 0.334208607673645\niter625 loss = 0.33415690064430237\niter626 loss = 0.33411189913749695\niter627 loss = 0.33406198024749756\niter628 loss = 0.3340177834033966\niter629 loss = 0.33396926522254944\niter630 loss = 0.33392637968063354\niter631 loss = 0.33387938141822815\niter632 loss = 0.33383700251579285\niter633 loss = 0.33379173278808594\niter634 loss = 0.3337503969669342\niter635 loss = 0.33370643854141235\niter636 loss = 0.33366623520851135\niter637 loss = 0.3336237370967865\niter638 loss = 0.333584725856781\niter639 loss = 0.3335436284542084\niter640 loss = 0.33350563049316406\niter641 loss = 0.333465576171875\niter642 loss = 0.33342844247817993\niter643 loss = 0.3333894908428192\niter644 loss = 0.33335310220718384\niter645 loss = 0.3333154618740082\niter646 loss = 0.3332800567150116\niter647 loss = 0.3332432806491852\niter648 loss = 0.3332083523273468\niter649 loss = 0.3331725299358368\niter650 loss = 0.33313843607902527\niter651 loss = 0.3331037163734436\niter652 loss = 0.3330704867839813\niter653 loss = 0.33303627371788025\niter654 loss = 0.33300378918647766\niter655 loss = 0.3329707384109497\niter656 loss = 0.33293870091438293\niter657 loss = 0.3329060673713684\niter658 loss = 0.33287495374679565\niter659 loss = 0.332843154668808\niter660 loss = 0.33281269669532776\niter661 loss = 0.33278167247772217\niter662 loss = 0.33275169134140015\niter663 loss = 0.33272141218185425\niter664 loss = 0.3326919674873352\niter665 loss = 0.332662433385849\niter666 loss = 0.332633376121521\niter667 loss = 0.33260440826416016\niter668 loss = 0.33257612586021423\niter669 loss = 0.3325473964214325\niter670 loss = 0.33251965045928955\niter671 loss = 0.3324914872646332\niter672 loss = 0.3324640989303589\niter673 loss = 0.33243653178215027\niter674 loss = 0.33240950107574463\niter675 loss = 0.3323825001716614\niter676 loss = 0.3323560059070587\niter677 loss = 0.3323291838169098\niter678 loss = 0.3323032855987549\niter679 loss = 0.3322770297527313\niter680 loss = 0.33225148916244507\niter681 loss = 0.3322257101535797\niter682 loss = 0.3322004973888397\niter683 loss = 0.3321751356124878\niter684 loss = 0.3321501910686493\niter685 loss = 0.33212533593177795\niter686 loss = 0.3321007788181305\niter687 loss = 0.33207619190216064\niter688 loss = 0.33205193281173706\niter689 loss = 0.332027792930603\niter690 loss = 0.3320038616657257\niter691 loss = 0.33198004961013794\niter692 loss = 0.33195656538009644\niter693 loss = 0.3319331407546997\niter694 loss = 0.33190956711769104\niter695 loss = 0.33188655972480774\niter696 loss = 0.3318636417388916\niter697 loss = 0.3318406045436859\niter698 loss = 0.33181828260421753\niter699 loss = 0.33179542422294617\niter700 loss = 0.33177319169044495\niter701 loss = 0.33175092935562134\niter702 loss = 0.3317286968231201\niter703 loss = 0.33170679211616516\niter704 loss = 0.33168479800224304\niter705 loss = 0.3316631317138672\niter706 loss = 0.33164140582084656\niter707 loss = 0.3316200077533722\niter708 loss = 0.33159875869750977\niter709 loss = 0.3315774202346802\niter710 loss = 0.3315563499927521\niter711 loss = 0.3315353989601135\niter712 loss = 0.3315146267414093\niter713 loss = 0.33149367570877075\niter714 loss = 0.33147311210632324\niter715 loss = 0.33145269751548767\niter716 loss = 0.3314323127269745\niter717 loss = 0.33141186833381653\niter718 loss = 0.33139166235923767\niter719 loss = 0.33137157559394836\niter720 loss = 0.3313514292240143\niter721 loss = 0.331331729888916\niter722 loss = 0.3313119411468506\niter723 loss = 0.3312923014163971\niter724 loss = 0.3312726318836212\niter725 loss = 0.3312530815601349\niter726 loss = 0.33123376965522766\niter727 loss = 0.33121442794799805\niter728 loss = 0.3311951756477356\niter729 loss = 0.33117613196372986\niter730 loss = 0.3311571776866913\niter731 loss = 0.3311382532119751\niter732 loss = 0.33111947774887085\niter733 loss = 0.33110061287879944\niter734 loss = 0.3310821056365967\niter735 loss = 0.3310634195804596\niter736 loss = 0.33104491233825684\niter737 loss = 0.3310267925262451\niter738 loss = 0.3310084044933319\niter739 loss = 0.33098992705345154\niter740 loss = 0.3309720456600189\niter741 loss = 0.330953985452652\niter742 loss = 0.3309360146522522\niter743 loss = 0.3309180438518524\niter744 loss = 0.33090025186538696\niter745 loss = 0.33088260889053345\niter746 loss = 0.330864816904068\niter747 loss = 0.3308474123477936\niter748 loss = 0.33082982897758484\niter749 loss = 0.33081257343292236\niter750 loss = 0.3307950496673584\niter751 loss = 0.33077770471572876\niter752 loss = 0.33076056838035583\niter753 loss = 0.33074337244033813\niter754 loss = 0.33072638511657715\niter755 loss = 0.3307095766067505\niter756 loss = 0.3306925892829895\niter757 loss = 0.3306756913661957\niter758 loss = 0.330658882856369\niter759 loss = 0.33064234256744385\niter760 loss = 0.3306267261505127\niter761 loss = 0.33061152696609497\niter762 loss = 0.3305952548980713\niter763 loss = 0.3305785655975342\niter764 loss = 0.33056214451789856\niter765 loss = 0.33054599165916443\niter766 loss = 0.3305310010910034\niter767 loss = 0.33051642775535583\niter768 loss = 0.33050021529197693\niter769 loss = 0.33048391342163086\niter770 loss = 0.3304685950279236\niter771 loss = 0.33045512437820435\niter772 loss = 0.33043479919433594\niter773 loss = 0.3304152488708496\niter774 loss = 0.33039581775665283\niter775 loss = 0.33037689328193665\niter776 loss = 0.3303582966327667\niter777 loss = 0.3303397595882416\niter778 loss = 0.3303220272064209\niter779 loss = 0.33030402660369873\niter780 loss = 0.33028650283813477\niter781 loss = 0.33026912808418274\niter782 loss = 0.3302520215511322\niter783 loss = 0.33023491501808167\niter784 loss = 0.3302180767059326\niter785 loss = 0.33020126819610596\niter786 loss = 0.3301847279071808\niter787 loss = 0.33016839623451233\niter788 loss = 0.33015188574790955\niter789 loss = 0.3301357924938202\niter790 loss = 0.33011969923973083\niter791 loss = 0.33010369539260864\niter792 loss = 0.330087810754776\niter793 loss = 0.33007189631462097\niter794 loss = 0.33005642890930176\niter795 loss = 0.33004069328308105\niter796 loss = 0.33002525568008423\niter797 loss = 0.3300098180770874\niter798 loss = 0.32999444007873535\niter799 loss = 0.32997927069664\niter800 loss = 0.3299641013145447\niter801 loss = 0.32994893193244934\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "iter802 loss = 0.3299340009689331\niter803 loss = 0.32991915941238403\niter804 loss = 0.3299042880535126\niter805 loss = 0.32988953590393066\niter806 loss = 0.32987475395202637\niter807 loss = 0.32986024022102356\niter808 loss = 0.3298455476760864\niter809 loss = 0.3298312723636627\niter810 loss = 0.32981669902801514\niter811 loss = 0.32980242371559143\niter812 loss = 0.3297881484031677\niter813 loss = 0.3297739326953888\niter814 loss = 0.3297598361968994\niter815 loss = 0.3297456204891205\niter816 loss = 0.32973167300224304\niter817 loss = 0.32971763610839844\niter818 loss = 0.32970383763313293\niter819 loss = 0.3296899199485779\niter820 loss = 0.32967618107795715\niter821 loss = 0.32966262102127075\niter822 loss = 0.32964885234832764\niter823 loss = 0.3296353220939636\niter824 loss = 0.3296215534210205\niter825 loss = 0.3296079933643341\niter826 loss = 0.3295947313308716\niter827 loss = 0.32958126068115234\niter828 loss = 0.32956790924072266\niter829 loss = 0.3295545279979706\niter830 loss = 0.3295412063598633\niter831 loss = 0.32952815294265747\niter832 loss = 0.3295149505138397\niter833 loss = 0.32950207591056824\niter834 loss = 0.32948875427246094\niter835 loss = 0.3294757008552551\niter836 loss = 0.32946285605430603\niter837 loss = 0.32944992184638977\niter838 loss = 0.329437255859375\niter839 loss = 0.32942432165145874\niter840 loss = 0.32941150665283203\niter841 loss = 0.3293986916542053\niter842 loss = 0.3293860852718353\niter843 loss = 0.3293735086917877\niter844 loss = 0.3293607532978058\niter845 loss = 0.32934829592704773\niter846 loss = 0.32933589816093445\niter847 loss = 0.329323410987854\niter848 loss = 0.32931095361709595\niter849 loss = 0.329298734664917\niter850 loss = 0.3292863070964813\niter851 loss = 0.3292739689350128\niter852 loss = 0.3292617201805115\niter853 loss = 0.3292495310306549\niter854 loss = 0.3292374908924103\niter855 loss = 0.3292253017425537\niter856 loss = 0.32921335101127625\niter857 loss = 0.32920119166374207\niter858 loss = 0.32918915152549744\niter859 loss = 0.32917729020118713\niter860 loss = 0.32916533946990967\niter861 loss = 0.32915347814559937\niter862 loss = 0.3291415572166443\niter863 loss = 0.32912954688072205\niter864 loss = 0.3291180431842804\niter865 loss = 0.3291062116622925\niter866 loss = 0.32909461855888367\niter867 loss = 0.329082727432251\niter868 loss = 0.3290712535381317\niter869 loss = 0.3290594816207886\niter870 loss = 0.32904794812202454\niter871 loss = 0.32903650403022766\niter872 loss = 0.329025000333786\niter873 loss = 0.32901352643966675\niter874 loss = 0.3290020227432251\niter875 loss = 0.3289908766746521\niter876 loss = 0.32897940278053284\niter877 loss = 0.3289681077003479\niter878 loss = 0.3289567530155182\niter879 loss = 0.3289455473423004\niter880 loss = 0.3289341330528259\niter881 loss = 0.32892313599586487\niter882 loss = 0.32891198992729187\niter883 loss = 0.3289007246494293\niter884 loss = 0.3288896381855011\niter885 loss = 0.32887840270996094\niter886 loss = 0.32886752486228943\niter887 loss = 0.32885637879371643\niter888 loss = 0.3288455605506897\niter889 loss = 0.3288344442844391\niter890 loss = 0.32882341742515564\niter891 loss = 0.3288125991821289\niter892 loss = 0.328801691532135\niter893 loss = 0.32879093289375305\niter894 loss = 0.32878005504608154\niter895 loss = 0.328769326210022\niter896 loss = 0.3287585973739624\niter897 loss = 0.3287477493286133\niter898 loss = 0.3287371098995209\niter899 loss = 0.3287266194820404\niter900 loss = 0.32871586084365845\niter901 loss = 0.328705370426178\niter902 loss = 0.32869473099708557\niter903 loss = 0.328684002161026\niter904 loss = 0.32867351174354553\niter905 loss = 0.32866308093070984\niter906 loss = 0.328652560710907\niter907 loss = 0.32864218950271606\niter908 loss = 0.328631728887558\niter909 loss = 0.32862135767936707\niter910 loss = 0.32861101627349854\niter911 loss = 0.3286006450653076\niter912 loss = 0.32859039306640625\niter913 loss = 0.32857996225357056\niter914 loss = 0.3285696506500244\niter915 loss = 0.3285594582557678\niter916 loss = 0.3285491466522217\niter917 loss = 0.328539103269577\niter918 loss = 0.32852891087532043\niter919 loss = 0.3285188376903534\niter920 loss = 0.3285086154937744\niter921 loss = 0.328498512506485\niter922 loss = 0.3284885287284851\niter923 loss = 0.3284784257411957\niter924 loss = 0.32846835255622864\niter925 loss = 0.32845842838287354\niter926 loss = 0.32844844460487366\niter927 loss = 0.3284386694431305\niter928 loss = 0.32842862606048584\niter929 loss = 0.3284185528755188\niter930 loss = 0.32840877771377563\niter931 loss = 0.3283991515636444\niter932 loss = 0.3283892869949341\niter933 loss = 0.32837945222854614\niter934 loss = 0.3283696174621582\niter935 loss = 0.3283599019050598\niter936 loss = 0.3283502757549286\niter937 loss = 0.3283405303955078\niter938 loss = 0.3283309042453766\niter939 loss = 0.3283211886882782\niter940 loss = 0.3283116817474365\niter941 loss = 0.32830187678337097\niter942 loss = 0.3282923400402069\niter943 loss = 0.32828283309936523\niter944 loss = 0.32827329635620117\niter945 loss = 0.32826387882232666\niter946 loss = 0.32825419306755066\niter947 loss = 0.32824474573135376\niter948 loss = 0.32823532819747925\niter949 loss = 0.3282260000705719\niter950 loss = 0.3282166123390198\niter951 loss = 0.3282071650028229\niter952 loss = 0.3281976878643036\niter953 loss = 0.3281885087490082\niter954 loss = 0.32817912101745605\niter955 loss = 0.32816970348358154\niter956 loss = 0.32816052436828613\niter957 loss = 0.3281514346599579\niter958 loss = 0.3281421363353729\niter959 loss = 0.3281330168247223\niter960 loss = 0.3281238079071045\niter961 loss = 0.3281148374080658\niter962 loss = 0.328105628490448\niter963 loss = 0.3280966281890869\niter964 loss = 0.3280874788761139\niter965 loss = 0.3280784785747528\niter966 loss = 0.328069269657135\niter967 loss = 0.3280602991580963\niter968 loss = 0.3280513286590576\niter969 loss = 0.3280423581600189\niter970 loss = 0.32803332805633545\niter971 loss = 0.32802462577819824\niter972 loss = 0.32801559567451477\niter973 loss = 0.3280065357685089\niter974 loss = 0.3279978036880493\niter975 loss = 0.3279888927936554\niter976 loss = 0.32798001170158386\niter977 loss = 0.32797130942344666\niter978 loss = 0.3279624581336975\niter979 loss = 0.3279534876346588\niter980 loss = 0.32794487476348877\niter981 loss = 0.3279360234737396\niter982 loss = 0.32792726159095764\niter983 loss = 0.32791879773139954\niter984 loss = 0.3279098868370056\niter985 loss = 0.32790130376815796\niter986 loss = 0.3278927803039551\niter987 loss = 0.3278842568397522\niter988 loss = 0.3278754949569702\niter989 loss = 0.327866792678833\niter990 loss = 0.32785865664482117\niter991 loss = 0.32785096764564514\niter992 loss = 0.3278424143791199\niter993 loss = 0.32783374190330505\niter994 loss = 0.3278253376483917\niter995 loss = 0.3278166949748993\niter996 loss = 0.327808141708374\niter997 loss = 0.32779964804649353\niter998 loss = 0.32779255509376526\niter999 loss = 0.3277848958969116\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "cb1ba6227b38b7785f43351e71bcb49a2bf69fd4"
      },
      "cell_type": "markdown",
      "source": "Plotting the cost function against no of iteration. The plt shows how the cost function is decreasing in each iteration."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9e6ad107b05ee8b6c1ba44e13b7941505e8b607"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nplt.scatter(iter1[20:],cost[20:])\nplt.show()",
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7faa78097b00>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAYy3CWpgf4/vQvcc4X+nPcAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b1c9de9ce48673ba7988f4a8cb7bb1dd3ee5a02d"
      },
      "cell_type": "markdown",
      "source": "Finally computing the accuracy fo our nn model .As  predicted is 2D Variable ,we first squeeze it to 1D then compare its data part with the target.data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65d44ff4c27b58e106a57519bffe13d130b31325"
      },
      "cell_type": "code",
      "source": "predicted = nn_model(train_x).type(dtype)\npredicted[predicted>=.5]=1\npredicted[predicted<.5]=0\npredicted = predicted.squeeze()\ntorch.sum(predicted.data==target.data)/310",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 127,
          "data": {
            "text/plain": "0.8193548387096774"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "40d62296e7134fd74c031ebede655b8ee443531b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}